{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Jupyter Notebook for OpenML dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "plt.rcParams['figure.dpi']= 120\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8 \n",
    "\n",
    "from preamble import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Random Forest model from the dataset and compute important features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_forest(dataset):    \n",
    "    data = oml.datasets.get_dataset(dataset) \n",
    "    X, y, features = data.get_data(target=data.default_target_attribute, return_attribute_names=True); \n",
    "    forest = Pipeline([('Imputer', preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
    "                       ('classifiers', RandomForestClassifier(n_estimators=100, random_state=0))])\n",
    "    forest.fit(X,y)\n",
    "    \n",
    "    importances = forest.steps[1][1].feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    return data.name, features, importances, indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Top-20 important features for the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(features, importances, indices):\n",
    "    a = 0.8\n",
    "    f_sub = []\n",
    "    max_features = 20\n",
    "\n",
    "    for f in range(min(len(features), max_features)): \n",
    "            f_sub.append(f)\n",
    "\n",
    "    # Create a figure of given size\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    # Set title\n",
    "    ttl = dataset_name\n",
    "\n",
    "    df = pd.DataFrame(importances[indices[f_sub]][::-1])\n",
    "    df.plot(kind='barh', ax=ax, alpha=a, legend=False, edgecolor='w', \n",
    "            title=ttl, color = [plt.cm.viridis(np.arange(len(df))*10)])\n",
    "\n",
    "    # Remove grid lines and plot frame\n",
    "    ax.grid(False)\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "    # Customize title\n",
    "    ax.set_title(ax.get_title(), fontsize=14, alpha=a, ha='left', x=0, y=1.0)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "    # Customize x tick lables\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    ax.locator_params(axis='x', tight=True, nbins=5)\n",
    "\n",
    "    # Customize y tick labels\n",
    "    yticks = np.array(features)[indices[f_sub]][::-1]\n",
    "    ax.set_yticklabels(yticks, fontsize=8, alpha=a)\n",
    "    ax.yaxis.set_tick_params(pad=2)\n",
    "    ax.yaxis.set_ticks_position('none')  \n",
    "    ax.set_ylim(ax.get_ylim()[0]-0.5, ax.get_ylim()[1]+0.5) \n",
    "\n",
    "    # Set x axis text\n",
    "    xlab = 'Feature importance'\n",
    "    ax.set_xlabel(xlab, fontsize=10, alpha=a)\n",
    "    ax.xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "    # Set y axis text\n",
    "    ylab = 'Feature'\n",
    "    ax.set_ylabel(ylab, fontsize=10, alpha=a)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose desired dataset and generate the most important plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name, features, importances, indices = build_forest(dataset)\n",
    "plot_feature_importances(features, importances, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Landmarking meta-features were calculated and stored in MongoDB: (Matthias Reif et al. 2012, Abdelmessih et al. 2010)\n",
    "\n",
    "The accuracy values of the following simple learners are used: Naive Bayes, Linear Discriminant Analysis, One-Nearest Neighbor, Decision Node, Random Node.\n",
    "\n",
    "- **Naive Bayes Learner** is a probabilistic classifier, based on Bayesâ€™ Theorem:\n",
    "$$ p(X|Y) = \\frac{p(Y|X) \\cdot p(X)}{p(Y)} $$\n",
    "\n",
    "    where p(X) is the prior probability and p(X|Y) is the posterior probability. It is called naive, because it\n",
    "    assumes independence of all attributes to each other.\n",
    "- **Linear Discriminant Learner** is a type of discriminant analysis, which is understood as the grouping and separation of categories according to specific features. Linear discriminant is basically finding a linear combination of features that separates the classes best. The resulting separation model is a line, a plane, or a hyperplane, depending on the number of features combined. \n",
    "\n",
    "- **One Nearest Neighbor Learner** is a classifier based on instance-based learning. A test point is assigned to the class of the nearest point within the training set. \n",
    "\n",
    "- **Decision Node Learner** is a classifier based on the information gain of attributes. The information gain indicates how informative an attribute is with respect to the classification task using its entropy. The higher the variability of the attribute values, the higher its information gain. This learner selects the attribute with the highest information gain. Then, it creates a single node decision tree consisting of the chosen attribute as a split node. \n",
    "\n",
    "- **Randomly Chosen Node Learner** is a classifier that results also in a single decision node, based on a randomly chosen attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connet_mongoclient(host):\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client.landmarkers\n",
    "    return db\n",
    "    \n",
    "def get_landmarkers_from_db():\n",
    "    db = connet_mongoclient('109.238.10.185')\n",
    "    collection = db.landmarkers2\n",
    "    df = pd.DataFrame(list(collection.find()))\n",
    "    \n",
    "    landmarkers = pd.DataFrame(df['score'].values.tolist())\n",
    "    datasetID = df['dataset'].astype(int)\n",
    "    datasets = oml.datasets.get_datasets(datasetID)\n",
    "    return df, landmarkers, datasetID, datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity between datasets were computed based on the distance function and stored in MongoDB: (Todorovski et al. 2000)\n",
    "    $$ dist(d_i, d_j) = \\sum_x{\\frac{|v_{x, d_i}-v_{x, d_j}|}{max_{k \\neq i}(v_x, d_k)-min_{k \\neq i}(v_x, d_k)}}$$\n",
    "\n",
    "where $d_i$ and $d_j$ are datasets, and $v_{x, d_i}$ is the value of meta-attribute $x$ for dataset $d_i$. The distance is divided by the range to normalize the values, so that all meta-attributes have the same range of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_from_db():\n",
    "    db = connet_mongoclient('109.238.10.185')\n",
    "    collection = db.distance\n",
    "    df = pd.DataFrame(list(collection.find()))\n",
    "    distance = list(df['distance'].values.flatten())\n",
    "    return distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similar_datasets(dataset):\n",
    "    n = 30\n",
    "    dataset_index = df.index[datasetID == dataset][0]\n",
    "    similar_dist = distance[:][dataset_index]\n",
    "    similar_index = np.argsort(similar_dist)[:n]\n",
    "    return similar_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_name(datasets, similar_index):\n",
    "    n = 30\n",
    "    datasets_name = []\n",
    "\n",
    "    for i in similar_index:\n",
    "        datasets_name.append(datasets[i].name)    \n",
    "    return datasets_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", font_scale=0.75)\n",
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "df, landmarkers, datasetID, datasets = get_landmarkers_from_db()\n",
    "landmarkers.columns = ['One-Nearest Neighbor', 'Linear Discriminant Analysis', 'Gaussian Naive Bayes', \n",
    "                       'Decision Node', 'Random Node']\n",
    "\n",
    "distance = np.squeeze(get_distance_from_db())\n",
    "similar_index = compute_similar_datasets(dataset)\n",
    "sns.violinplot(data=landmarkers.iloc[similar_index], palette=\"Set3\", bw=.2, cut=1, linewidth=1)\n",
    "sns.despine(left=True, bottom=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_name = get_datasets_name(datasets, similar_index)\n",
    "sns.set(style=\"white\")\n",
    "corr = pd.DataFrame(distance[similar_index[:, None], similar_index], \n",
    "                    index = datasets_name, columns = datasets_name)\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.heatmap(corr, mask=mask, cmap = \"BuPu_r\", vmax= 1,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
