{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Automatic Jupyter Notebook for OpenML dataset 1: anneal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import openml as oml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import dummy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "plt.rcParams['figure.dpi']= 120\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8 \n",
    "\n",
    "from preamble import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of unique values for the default target attribute in this data set is 0.0056.\n",
    "Because this is lower or equal than 5% of the dataset we assume that this is a **classification** problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate baseline accuracy for classification problems using scikit-learn DummyClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(data):\n",
    "    strategies = ['stratified','most_frequent','prior','uniform']\n",
    "    baseDict = {}\n",
    "    X, y, features = data.get_data(target=data.default_target_attribute, return_attribute_names=True); \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "    for strat in strategies:\n",
    "        clf = dummy.DummyClassifier(strategy=strat,random_state=0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        baseDict[strat] = clf.score(X_test, y_test)\n",
    "    return baseDict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates a plot of the classification baseline accuracy of the various baseline strategies using scikit-learn DummyClassifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baseline(scores):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    from collections import namedtuple\n",
    "\n",
    "    strats = scores\n",
    "    maxBaseline = strats[max(strats, key=strats.get)]\n",
    "    \n",
    "    n_groups = len(strats)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.1\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    plt.bar(range(len(strats)), strats.values(), align='center')\n",
    "    plt.xticks(range(len(strats)), list(strats.keys()))\n",
    "    plt.yticks(np.arange(0, 1.1, step=0.2))\n",
    "    plt.yticks(list(plt.yticks()[0]) + [maxBaseline])\n",
    "\n",
    "    ax.set_ylim(ymin=0)\n",
    "    ax.set_ylim(ymax=1)\n",
    "    ax.set_xlabel('Baseline Strategy')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Baseline Performance Predicting Feature: ' + data.default_target_attribute)\n",
    "    plt.axhline(y=maxBaseline, color='r', linestyle='--', label=maxBaseline)\n",
    "    plt.gca().get_yticklabels()[6].set_color('red')\n",
    "    fig.tight_layout()\n",
    "    plt.show() \n",
    "    return maxBaseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates a plot of the accuracy of the machinelearning algorithms against the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alg(scores, maxBaseline):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    from collections import namedtuple\n",
    "\n",
    "    strats = scores\n",
    "    \n",
    "    n_groups = len(strats)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.1\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    barlist =plt.bar(range(len(strats)), strats.values(), align='center')\n",
    "    plt.xticks(range(len(strats)), list(strats.keys()))\n",
    "    plt.yticks(np.arange(0, 1.1, step=0.2))\n",
    "    plt.yticks(list(plt.yticks()[0]) + [maxBaseline])\n",
    "\n",
    "    ax.set_ylim(ymin=0)\n",
    "    ax.set_ylim(ymax=1)\n",
    "    ax.set_xlabel('Machine Learning Algorithm')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Algorithm Performance Predicting Feature: ' + data.default_target_attribute)\n",
    "    plt.axhline(y=maxBaseline, color='r', linestyle='--', label=maxBaseline)\n",
    "    plt.gca().get_yticklabels()[6].set_color('red')\n",
    "    for bar in barlist:\n",
    "        if bar.get_height() > maxBaseline:\n",
    "            bar.set_facecolor('g') \n",
    "    fig.tight_layout()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Random Forest model from the dataset and compute important features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_forest(data):    \n",
    "    X, y, features = data.get_data(target=data.default_target_attribute, return_attribute_names=True); \n",
    "    forest = Pipeline([('Imputer', preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
    "                       ('classifiers', RandomForestClassifier(n_estimators=100, random_state=0))])\n",
    "    forest.fit(X,y)\n",
    "    \n",
    "    importances = forest.steps[1][1].feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    return data.name, features, importances, indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Top-20 important features for the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(features, importances, indices):\n",
    "    a = 0.8\n",
    "    f_sub = []\n",
    "    max_features = 20\n",
    "\n",
    "    for f in range(min(len(features), max_features)): \n",
    "            f_sub.append(f)\n",
    "\n",
    "    # Create a figure of given size\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    # Set title\n",
    "    ttl = dataset_name\n",
    "\n",
    "    df = pd.DataFrame(importances[indices[f_sub]][::-1])\n",
    "    df.plot(kind='barh', ax=ax, alpha=a, legend=False, edgecolor='w', \n",
    "            title=ttl, color = [plt.cm.viridis(np.arange(len(df))*10)])\n",
    "\n",
    "    # Remove grid lines and plot frame\n",
    "    ax.grid(False)\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "    # Customize title\n",
    "    ax.set_title(ax.get_title(), fontsize=14, alpha=a, ha='left', x=0, y=1.0)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "    # Customize x tick lables\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    ax.locator_params(axis='x', tight=True, nbins=5)\n",
    "\n",
    "    # Customize y tick labels\n",
    "    yticks = np.array(features)[indices[f_sub]][::-1]\n",
    "    ax.set_yticklabels(yticks, fontsize=8, alpha=a)\n",
    "    ax.yaxis.set_tick_params(pad=2)\n",
    "    ax.yaxis.set_ticks_position('none')  \n",
    "    ax.set_ylim(ax.get_ylim()[0]-0.5, ax.get_ylim()[1]+0.5) \n",
    "\n",
    "    # Set x axis text\n",
    "    xlab = 'Feature importance'\n",
    "    ax.set_xlabel(xlab, fontsize=10, alpha=a)\n",
    "    ax.xaxis.set_label_coords(0.5, -0.1)\n",
    "\n",
    "    # Set y axis text\n",
    "    ylab = 'Feature'\n",
    "    ax.set_ylabel(ylab, fontsize=10, alpha=a)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose desired dataset and generate the most important plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = oml.datasets.get_dataset(dataset)\n",
    "dataset_name, features, importances, indices = build_forest(data)\n",
    "plot_feature_importances(features, importances, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the classification baseline acuracy of the various baseline strategies using scikit-learn DummyClassifier.\n",
    "\n",
    "The target feature is: **class**\n",
    "\n",
    "The following baseline strategies are used: stratified, most_frequent, prior, uniform.\n",
    "\n",
    "The strategies work as follow according to the sciki-learn API:\n",
    "\n",
    "- **stratified**: Generates predictions by respecting the training setâ€™s class distribution.\n",
    "\n",
    "- **most_frequent**: Always predicts the most frequent label in the training set. Also known as ZeroR.\n",
    "\n",
    "- **prior**: Always predicts the class that maximizes the class prior. \n",
    "\n",
    "- **uniform**: Generates predictions uniformly at random.\n",
    "\n",
    "The horizontal red dotted line denotes the baseline value for this dataset which is equal to the best performing baseline strategy.\n",
    "\n",
    "[More information.](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxBaseline = plot_baseline(baseline(data))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the decision tree algorithm on the dataset WIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the decision tree algorithm on the dataset\n",
    "from sklearn import tree\n",
    "#Running default values, it is recommended to experiment with the values of the parameters below. Try min_samples_leaf=5\n",
    "clf = tree.DecisionTreeClassifier(max_depth=None, min_samples_leaf=1, max_features=None, max_leaf_nodes=None)\n",
    "X, y, features = data.get_data(target=data.default_target_attribute, return_attribute_names=True); \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "p = len(features)\n",
    "n = len(X_train)\n",
    "#computational complexity O(n^2 * p)\n",
    "complexity = n**2 * p\n",
    "\n",
    "if complexity <= 50000000000000:\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    strats = baseline(data)\n",
    "    strats['Decision Tree'] = acc \n",
    "else: \n",
    "    print(\"computation complexity too high, please run manually if desired.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the multinomial naive bayes algorithm on the dataset WIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the Multinomial Naive Bayes algorithm on the dataset\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Running default values, it is recommended to experiment with the values of the parameters below.\n",
    "clf = MultinomialNB()\n",
    "X, y, features = data.get_data(target=data.default_target_attribute, return_attribute_names=True); \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "p = len(features)\n",
    "n = len(X_train)\n",
    "#computational complexity O(n * p)\n",
    "complexity = n * p\n",
    "\n",
    "if complexity <= 50000000000000:\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    strats['naive bayes'] = acc \n",
    "else: \n",
    "    print(\"computation complexity too high, please run manually if desired.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the random forest algorithm on the dataset WIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the Random Forest algorithm on the dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import math\n",
    "#Running default values, it is recommended to experiment with the values of the parameters below.\n",
    "clf = RandomForestClassifier()\n",
    "X, y, features = data.get_data(target=data.default_target_attribute, return_attribute_names=True); \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "p = len(features)\n",
    "n = len(X_train)\n",
    "#computational complexity O(n^2 * sqrt(p * n_trees))\n",
    "complexity = n**2 * math.sqrt(p * 10)\n",
    "\n",
    "if complexity <= 50000000000000:\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    strats['random forest'] = acc \n",
    "else: \n",
    "    print(\"computation complexity too high, please run manually if desired.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the classification support vector algorithm on the dataset WIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the Classification Support Vector Machine algorithm on the dataset\n",
    "from sklearn import svm\n",
    "#Running default values, it is recommended to experiment with the values of the parameters below.\n",
    "clf = svm.SVC()\n",
    "X, y, features = data.get_data(target=data.default_target_attribute, return_attribute_names=True); \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "p = len(features)\n",
    "n = len(X_train)\n",
    "#computational complexity O(n^2 * p + n^3)\n",
    "complexity = n**2 * p + n**3\n",
    "\n",
    "if complexity <= 50000000000000:\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    strats['support vector machine'] = acc \n",
    "else: \n",
    "    print(\"computation complexity too high, please run manually if desired.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy of various machine learning algorithms against the baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_alg(strats, maxBaseline) "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
